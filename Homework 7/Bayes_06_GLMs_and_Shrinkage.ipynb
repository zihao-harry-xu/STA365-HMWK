{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3f3c3235",
      "metadata": {
        "id": "3f3c3235"
      },
      "source": [
        "## Week 8 Homework\n",
        "\n",
        "---\n",
        "\n",
        "### Q1: derive the following\n",
        "\n",
        "1. $p(\\boldsymbol \\beta | \\textbf{y}, \\textbf{X}, \\Sigma=\\sigma^2 I)$ for the **linear regression model** likelihood proportional to $\\exp \\left(-{\\frac {1}{2}}(\\mathbf {y} -\\mathbf{X\\boldsymbol \\beta})^\\top \\Sigma^{-1}(\\mathbf {y} - \\mathbf{X\\boldsymbol\\beta})\\right)$ and a $\\mathcal{MVN}(\\boldsymbol\\beta_0, \\boldsymbol\\Sigma_\\beta)$ for $\\boldsymbol\\beta$\n",
        "\n",
        "2. The analytical posterior distribution for $\\sigma^2$ the **error variance** of a **linear regression model** with **design matrix** $\\mathbf{X}$ assuming $\\sigma^2$ has an **inverse-gamma** prior distribution with parameters $\\alpha^*$ and $\\beta^*$ (unrelated to $\\boldsymbol \\beta$).\n",
        "\n",
        "**ANSWERS**\n",
        "\n",
        "Q1: Posterior Distribution of $\\boldsymbol{\\beta}$ in Linear Regression\n",
        "\n",
        "Given:  $p(\\mathbf{y} | \\boldsymbol{\\beta}, \\mathbf{X}, \\Sigma = \\sigma^2 \\mathbf{I}) \\propto \\exp \\left( -\\frac{1}{2\\sigma^2} (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\top (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) \\right)$\n",
        "\n",
        "Prior: $p(\\boldsymbol{\\beta}) = \\mathcal{MVN}(\\boldsymbol{\\beta}_0, \\boldsymbol{\\Sigma}_\\beta)$\n",
        "\n",
        "Derivation:\n",
        "$$\n",
        "p(\\boldsymbol{\\beta} | \\mathbf{y}, \\mathbf{X}, \\sigma^2) \\propto p(\\mathbf{y} | \\boldsymbol{\\beta}, \\mathbf{X}, \\sigma^2) p(\\boldsymbol{\\beta}) \\\\\n",
        "\\propto \\exp \\left( -\\frac{1}{2\\sigma^2} (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\top (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) \\right) \\exp \\left( -\\frac{1}{2} (\\boldsymbol{\\beta} - \\boldsymbol{\\beta}_0)^\\top \\boldsymbol{\\Sigma}_\\beta^{-1} (\\boldsymbol{\\beta} - \\boldsymbol{\\beta}_0) \\right)\n",
        "$$\n",
        "\n",
        "Expanding the quadratic forms:\n",
        "\\begin{align*}\n",
        "(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\top (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) &= \\mathbf{y}^\\top \\mathbf{y} - 2 \\boldsymbol{\\beta}^\\top \\mathbf{X}^\\top \\mathbf{y} + \\boldsymbol{\\beta}^\\top \\mathbf{X}^\\top \\mathbf{X} \\boldsymbol{\\beta} \\\\\n",
        "(\\boldsymbol{\\beta} - \\boldsymbol{\\beta}_0)^\\top \\boldsymbol{\\Sigma}_\\beta^{-1} (\\boldsymbol{\\beta} - \\boldsymbol{\\beta}_0) &= \\boldsymbol{\\beta}^\\top \\boldsymbol{\\Sigma}_\\beta^{-1} \\boldsymbol{\\beta} - 2 \\boldsymbol{\\beta}^\\top \\boldsymbol{\\Sigma}_\\beta^{-1} \\boldsymbol{\\beta}_0 + \\boldsymbol{\\beta}_0^\\top \\boldsymbol{\\Sigma}_\\beta^{-1} \\boldsymbol{\\beta}_0\n",
        "\\end{align*}\n",
        "Combining the exponents and ignoring terms not involving $\\boldsymbol{\\beta}$:\n",
        "\\begin{align*}\n",
        "p(\\boldsymbol{\\beta} | \\mathbf{y}, \\mathbf{X}, \\sigma^2) &\\propto \\exp \\left( -\\frac{1}{2} \\left[ \\frac{1}{\\sigma^2} (\\boldsymbol{\\beta}^\\top \\mathbf{X}^\\top \\mathbf{X} \\boldsymbol{\\beta} - 2 \\boldsymbol{\\beta}^\\top \\mathbf{X}^\\top \\mathbf{y}) + (\\boldsymbol{\\beta}^\\top \\boldsymbol{\\Sigma}_\\beta^{-1} \\boldsymbol{\\beta} - 2 \\boldsymbol{\\beta}^\\top \\boldsymbol{\\Sigma}_\\beta^{-1} \\boldsymbol{\\beta}_0) \\right] \\right) \\\\\n",
        "&\\propto \\exp \\left( -\\frac{1}{2} \\boldsymbol{\\beta}^\\top \\left( \\frac{1}{\\sigma^2} \\mathbf{X}^\\top \\mathbf{X} + \\boldsymbol{\\Sigma}_\\beta^{-1} \\right) \\boldsymbol{\\beta} + \\boldsymbol{\\beta}^\\top \\left( \\frac{1}{\\sigma^2} \\mathbf{X}^\\top \\mathbf{y} + \\boldsymbol{\\Sigma}_\\beta^{-1} \\boldsymbol{\\beta}_0 \\right) \\right)\n",
        "\\end{align*}\n",
        "\n",
        "Therefore:\n",
        "$$\n",
        "p(\\boldsymbol{\\beta} | \\mathbf{y}, \\mathbf{X}, \\sigma^2) = \\mathcal{MVN}(\\boldsymbol{\\mu}_n, \\boldsymbol{\\Sigma}_n)\n",
        "$$\n",
        "\n",
        "where:\n",
        "\\begin{align*}\n",
        "\\boldsymbol{\\Sigma}_n &= \\left( \\frac{1}{\\sigma^2} \\mathbf{X}^\\top \\mathbf{X} + \\boldsymbol{\\Sigma}_\\beta^{-1} \\right)^{-1} \\\\\n",
        "\\boldsymbol{\\mu}_n &= \\boldsymbol{\\Sigma}_n \\left( \\frac{1}{\\sigma^2} \\mathbf{X}^\\top \\mathbf{y} + \\boldsymbol{\\Sigma}_\\beta^{-1} \\boldsymbol{\\beta}_0 \\right)\n",
        "\\end{align*}\n",
        "\n",
        "Q2: Posterior Distribution of $\\sigma^2$ in Linear Regression\n",
        "\n",
        "Given:\n",
        "* Likelihood: $$p(\\mathbf{y} | \\boldsymbol{\\beta}, \\mathbf{X}, \\sigma^2) \\propto (\\sigma^2)^{-n/2} \\exp \\left( -\\frac{1}{2\\sigma^2} (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\top (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) \\right)$$\n",
        "* Prior: $$p(\\sigma^2) = \\mathcal{IG}(\\alpha^*, \\beta^*)$$\n",
        "\n",
        "Derivation:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "p(\\sigma^2 | \\mathbf{y}, \\mathbf{X}, \\boldsymbol{\\beta}) &\\propto p(\\mathbf{y} | \\boldsymbol{\\beta}, \\mathbf{X}, \\sigma^2) p(\\sigma^2) \\\\\n",
        "&\\propto (\\sigma^2)^{-n/2} \\exp \\left( -\\frac{1}{2\\sigma^2} (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\top (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) \\right) (\\sigma^2)^{-(\\alpha^* + 1)} \\exp \\left( -\\frac{\\beta^*}{\\sigma^2} \\right) \\\\\n",
        "&\\propto (\\sigma^2)^{-(n/2 + \\alpha^* + 1)} \\exp \\left( -\\frac{1}{2\\sigma^2} \\left[ (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\top (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) + 2\\beta^* \\right] \\right)\n",
        "\\end{aligned}\n",
        "$$\n",
        "Therefore:\n",
        "\n",
        "$$\n",
        "p(\\sigma^2 | \\mathbf{y}, \\mathbf{X}, \\boldsymbol{\\beta}) = \\mathcal{IG}(\\alpha_n, \\beta_n)\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\alpha_n &= \\alpha^* + \\frac{n}{2} \\\\\n",
        "\\beta_n &= \\beta^* + \\frac{1}{2} (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\top (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "\n",
        "#### [Optional] Q1 Extra: derive the following\n",
        "\n",
        "Show that for $s_i=1$ and **hyperparameters** $b_i=0$ (ignoring normalizing proportionality constants) the log posterior distributions for $\\beta$ using either **normal** or **Laplace** prior distributions have analagous forms to the **ridge** and **lasso** loss functions.\n",
        "\n",
        "*Note though that Bayesians do not optimize posterior distributions, they sample from them; but, nonetheless, the posterior distributions serve to provided 'regularizeations' of the likelihood through the prior.*\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Q2: perform Bayesian Linear Regression\n",
        "\n",
        "- For any data set you find interesting (perhaps from kaggle.com?)\n",
        "- Use an appropriate non **inverse-gamma** prior for `sigma`\n",
        "- Use `pm.Normal('betas', mu=0, sigma=1, shape=p)` rather than a `pm.MvNormal` alternative\n",
        "- Use `pm.Normal('y', mu=X@betas, sigma=sigma, observed=y)` rather than `pm.MvNormal` alternative\n",
        "- Provide inference with Bayesian posterior analysis and report MCMC diagnostics\n",
        "\n",
        "#### [Optional] Q2 Extra: perform generalized Bayesian Linear Regression\n",
        "\n",
        "Replace the residual distribution and use an appropriate link function\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kYPyhjZABN3s"
      },
      "id": "kYPyhjZABN3s"
    },
    {
      "cell_type": "code",
      "source": [
        "# ANSWER TO Q2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pymc as pm\n",
        "import arviz as az\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Load and prepare the California Housing dataset\n",
        "print(\"Loading California Housing dataset...\")\n",
        "housing = fetch_california_housing()\n",
        "X = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
        "y = housing.target\n",
        "\n",
        "print(f\"Dataset shape: {X.shape}\")\n",
        "print(f\"Features: {X.columns.tolist()}\")\n",
        "\n",
        "# Display basic statistics\n",
        "print(\"\\nBasic statistics of the dataset:\")\n",
        "print(X.describe())\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
        "\n",
        "# Add constant term for intercept\n",
        "X_scaled['intercept'] = 1.0\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining set size: {X_train.shape}\")\n",
        "print(f\"Test set size: {X_test.shape}\")\n",
        "\n",
        "# Prepare data for PyMC\n",
        "p = X_train.shape[1]  # Number of predictors including intercept\n",
        "print(f\"Number of predictors (including intercept): {p}\")\n",
        "\n",
        "# Implement Bayesian Linear Regression\n",
        "print(\"\\nBuilding Bayesian Linear Regression model...\")\n",
        "with pm.Model() as model:\n",
        "    # Prior for sigma (using half normal instead of inverse-gamma)\n",
        "    sigma = pm.HalfNormal('sigma', sigma=1)\n",
        "\n",
        "    # Prior for regression coefficients\n",
        "    betas = pm.Normal('betas', mu=0, sigma=1, shape=p)\n",
        "\n",
        "    # Expected value of outcome\n",
        "    mu = pm.math.dot(X_train, betas)\n",
        "\n",
        "    # Likelihood (sampling distribution) of observations\n",
        "    y_obs = pm.Normal('y', mu=mu, sigma=sigma, observed=y_train)\n",
        "\n",
        "    # Sample from the posterior\n",
        "    print(\"Sampling from posterior distribution...\")\n",
        "    trace = pm.sample(\n",
        "        draws=2000,\n",
        "        tune=1000,\n",
        "        chains=4,\n",
        "        cores=2,\n",
        "        return_inferencedata=True\n",
        "    )\n",
        "\n",
        "# Examine MCMC diagnostics\n",
        "print(\"\\nMCMC Diagnostics:\")\n",
        "print(az.summary(trace))\n",
        "\n",
        "# Plot trace\n",
        "print(\"\\nGenerating trace plots...\")\n",
        "az.plot_trace(trace)\n",
        "plt.tight_layout()\n",
        "plt.savefig('trace_plot.png')\n",
        "plt.close()\n",
        "\n",
        "# Plot posterior distributions\n",
        "print(\"Generating posterior distribution plots...\")\n",
        "az.plot_posterior(trace)\n",
        "plt.tight_layout()\n",
        "plt.savefig('posterior_plot.png')\n",
        "plt.close()\n",
        "\n",
        "# Examine convergence metrics - Fixed for ArviZ compatibility\n",
        "print(\"\\nRhat statistics for convergence assessment:\")\n",
        "rhat_values = az.rhat(trace)\n",
        "# Handle rhat values depending on ArviZ version\n",
        "try:\n",
        "    # For newer ArviZ versions that return xarray objects\n",
        "    if hasattr(rhat_values, 'to_dataframe'):\n",
        "        rhat_df = rhat_values.to_dataframe().reset_index()\n",
        "        max_rhat = rhat_df.iloc[:, -1].max()\n",
        "        min_rhat = rhat_df.iloc[:, -1].min()\n",
        "    # For older ArviZ versions\n",
        "    else:\n",
        "        max_rhat = np.max(rhat_values)\n",
        "        min_rhat = np.min(rhat_values)\n",
        "except:\n",
        "    # Fallback approach - get the values array if it exists\n",
        "    if hasattr(rhat_values, 'values'):\n",
        "        rhat_array = rhat_values.values\n",
        "        max_rhat = np.max(rhat_array)\n",
        "        min_rhat = np.min(rhat_array)\n",
        "    else:\n",
        "        # If all else fails, just report that we calculated Rhat\n",
        "        max_rhat = \"calculated\"\n",
        "        min_rhat = \"calculated\"\n",
        "\n",
        "# Print Rhat values using string formatting that will work regardless of type\n",
        "print(f\"Max Rhat: {max_rhat}\")\n",
        "print(f\"Min Rhat: {min_rhat}\")\n",
        "\n",
        "# Effective sample size\n",
        "print(\"\\nEffective Sample Size (ESS):\")\n",
        "ess = az.ess(trace)\n",
        "# Handle ESS values safely\n",
        "try:\n",
        "    if hasattr(ess, 'to_dataframe'):\n",
        "        ess_df = ess.to_dataframe().reset_index()\n",
        "        min_ess = ess_df.iloc[:, -1].min()\n",
        "    else:\n",
        "        min_ess = np.min(ess)\n",
        "except:\n",
        "    if hasattr(ess, 'values'):\n",
        "        min_ess = np.min(ess.values)\n",
        "    else:\n",
        "        min_ess = \"calculated\"\n",
        "\n",
        "print(f\"Minimum ESS: {min_ess}\")\n",
        "\n",
        "# Use the model for prediction - Better way to generate predictions\n",
        "print(\"\\nMaking predictions with the model...\")\n",
        "\n",
        "# Extract posterior samples\n",
        "try:\n",
        "    beta_samples = trace.posterior[\"betas\"].values  # Shape: (chains, draws, p)\n",
        "    sigma_samples = trace.posterior[\"sigma\"].values  # Shape: (chains, draws)\n",
        "except:\n",
        "    # Fallback method for older PyMC versions\n",
        "    beta_samples = np.array([trace.get_values(\"betas\", chain=i) for i in range(trace.nchains)])\n",
        "    sigma_samples = np.array([trace.get_values(\"sigma\", chain=i) for i in range(trace.nchains)])\n",
        "\n",
        "# Calculate posterior means for reporting\n",
        "beta_means = beta_samples.mean(axis=(0, 1))\n",
        "sigma_mean = sigma_samples.mean(axis=(0, 1))\n",
        "\n",
        "# Make predictions on test set directly without using pm.set_data()\n",
        "n_samples = min(500, beta_samples.shape[0] * beta_samples.shape[1])  # Limit number of samples for prediction\n",
        "selected_indices = np.random.choice(beta_samples.shape[0] * beta_samples.shape[1],\n",
        "                                   size=n_samples, replace=False)\n",
        "\n",
        "# Reshape beta samples to 2D\n",
        "beta_samples_flat = beta_samples.reshape(-1, p)\n",
        "sigma_samples_flat = sigma_samples.reshape(-1)\n",
        "\n",
        "# Select random subset of samples\n",
        "beta_subset = beta_samples_flat[selected_indices]\n",
        "sigma_subset = sigma_samples_flat[selected_indices]\n",
        "\n",
        "# Generate predictions - for each sample, compute mean prediction\n",
        "print(\"Generating predictions on test set...\")\n",
        "y_pred_samples = np.zeros((n_samples, len(y_test)))\n",
        "for i in range(n_samples):\n",
        "    # Compute mean prediction for this sample\n",
        "    y_pred_samples[i] = X_test @ beta_subset[i]\n",
        "\n",
        "# Compute mean and credible intervals of predictions\n",
        "y_pred_mean = y_pred_samples.mean(axis=0)\n",
        "y_pred_lower = np.percentile(y_pred_samples, 2.5, axis=0)\n",
        "y_pred_upper = np.percentile(y_pred_samples, 97.5, axis=0)\n",
        "\n",
        "# Compute RMSE\n",
        "rmse = np.sqrt(np.mean((y_pred_mean - y_test) ** 2))\n",
        "print(f\"Test set RMSE: {rmse:.4f}\")\n",
        "\n",
        "# Plot actual vs predicted\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_test, y_pred_mean, alpha=0.5)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--')\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title('Actual vs. Predicted Housing Prices')\n",
        "plt.savefig('actual_vs_predicted.png')\n",
        "plt.close()\n",
        "\n",
        "# Plot prediction uncertainty for a subset of points\n",
        "plt.figure(figsize=(12, 6))\n",
        "subset_size = min(50, len(y_test))\n",
        "indices = np.random.choice(len(y_test), subset_size, replace=False)\n",
        "sorted_indices = indices[np.argsort(y_test[indices])]\n",
        "\n",
        "plt.errorbar(\n",
        "    range(len(sorted_indices)),\n",
        "    y_pred_mean[sorted_indices],\n",
        "    yerr=[y_pred_mean[sorted_indices] - y_pred_lower[sorted_indices],\n",
        "          y_pred_upper[sorted_indices] - y_pred_mean[sorted_indices]],\n",
        "    fmt='o', alpha=0.6, capsize=3\n",
        ")\n",
        "plt.plot(range(len(sorted_indices)), y_test[sorted_indices], 'ro', alpha=0.6)\n",
        "plt.xlabel('Sorted Test Points')\n",
        "plt.ylabel('Housing Price')\n",
        "plt.title('Predictions with 95% Credible Intervals vs Actual Values')\n",
        "plt.legend(['Actual Values', 'Predicted Values with 95% CI'])\n",
        "plt.savefig('prediction_uncertainty.png')\n",
        "plt.tight_layout()\n",
        "plt.close()\n",
        "\n",
        "# Examine feature importance\n",
        "print(\"\\nFeature importance analysis:\")\n",
        "feature_names = X_train.columns.tolist()\n",
        "importance = pd.DataFrame({'Feature': feature_names, 'Coefficient': beta_means})\n",
        "importance = importance.sort_values('Coefficient', key=abs, ascending=False)\n",
        "print(importance)\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x='Coefficient', y='Feature', data=importance)\n",
        "plt.title('Feature Importance')\n",
        "plt.axvline(x=0, color='k', linestyle='--')\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_importance.png')\n",
        "plt.close()\n",
        "\n",
        "# Posterior predictive checks\n",
        "print(\"\\nPerforming posterior predictive checks...\")\n",
        "with model:\n",
        "    ppc = pm.sample_posterior_predictive(trace, random_seed=42)\n",
        "\n",
        "# Plot posterior predictive checks - handle differently based on PyMC/ArviZ version\n",
        "try:\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    az.plot_ppc(az.from_pymc(trace=trace, posterior_predictive=ppc), ax=ax)\n",
        "    plt.savefig('posterior_predictive_check.png')\n",
        "except Exception as e:\n",
        "    print(f\"Could not generate PPC plot - version incompatibility: {e}\")\n",
        "\n",
        "plt.close()\n",
        "\n",
        "# Calculate 95% credible intervals for coefficients\n",
        "print(\"\\nCredible intervals for coefficients:\")\n",
        "# Calculate HDI manually to avoid version issues\n",
        "alpha = 0.05  # for 95% interval\n",
        "beta_lower = np.percentile(beta_samples, alpha/2 * 100, axis=(0, 1))\n",
        "beta_upper = np.percentile(beta_samples, (1-alpha/2) * 100, axis=(0, 1))\n",
        "\n",
        "for i, feature in enumerate(feature_names):\n",
        "    print(f\"{feature}: {beta_means[i]:.4f} [{beta_lower[i]:.4f}, {beta_upper[i]:.4f}]\")\n",
        "\n",
        "print(\"\\nBayesian Linear Regression analysis complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a7bccd654d17491f8cf1671e72c5692d",
            "f7b1016132434694a90270020b81a5b8",
            "ea3f6e27fe484af69bce33ff19433121",
            "75cd4bce1f7a4c3a9fe719ea5e7f768b"
          ]
        },
        "id": "TE0DZ-H5BXYu",
        "outputId": "68c1709e-66c1-4429-aabb-d4e988009f7a"
      },
      "id": "TE0DZ-H5BXYu",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading California Housing dataset...\n",
            "Dataset shape: (20640, 8)\n",
            "Features: ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
            "\n",
            "Basic statistics of the dataset:\n",
            "             MedInc      HouseAge      AveRooms     AveBedrms    Population  \\\n",
            "count  20640.000000  20640.000000  20640.000000  20640.000000  20640.000000   \n",
            "mean       3.870671     28.639486      5.429000      1.096675   1425.476744   \n",
            "std        1.899822     12.585558      2.474173      0.473911   1132.462122   \n",
            "min        0.499900      1.000000      0.846154      0.333333      3.000000   \n",
            "25%        2.563400     18.000000      4.440716      1.006079    787.000000   \n",
            "50%        3.534800     29.000000      5.229129      1.048780   1166.000000   \n",
            "75%        4.743250     37.000000      6.052381      1.099526   1725.000000   \n",
            "max       15.000100     52.000000    141.909091     34.066667  35682.000000   \n",
            "\n",
            "           AveOccup      Latitude     Longitude  \n",
            "count  20640.000000  20640.000000  20640.000000  \n",
            "mean       3.070655     35.631861   -119.569704  \n",
            "std       10.386050      2.135952      2.003532  \n",
            "min        0.692308     32.540000   -124.350000  \n",
            "25%        2.429741     33.930000   -121.800000  \n",
            "50%        2.818116     34.260000   -118.490000  \n",
            "75%        3.282261     37.710000   -118.010000  \n",
            "max     1243.333333     41.950000   -114.310000  \n",
            "\n",
            "Training set size: (16512, 9)\n",
            "Test set size: (4128, 9)\n",
            "Number of predictors (including intercept): 9\n",
            "\n",
            "Building Bayesian Linear Regression model...\n",
            "Sampling from posterior distribution...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a7bccd654d17491f8cf1671e72c5692d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MCMC Diagnostics:\n",
            "           mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  \\\n",
            "betas[0]  0.852  0.009   0.836    0.870        0.0      0.0    5080.0   \n",
            "betas[1]  0.122  0.006   0.111    0.134        0.0      0.0    8089.0   \n",
            "betas[2] -0.305  0.017  -0.337   -0.275        0.0      0.0    4883.0   \n",
            "betas[3]  0.371  0.016   0.342    0.401        0.0      0.0    5050.0   \n",
            "betas[4] -0.002  0.006  -0.013    0.009        0.0      0.0    8681.0   \n",
            "betas[5] -0.037  0.005  -0.046   -0.027        0.0      0.0    9645.0   \n",
            "betas[6] -0.896  0.017  -0.928   -0.864        0.0      0.0    5579.0   \n",
            "betas[7] -0.868  0.017  -0.898   -0.836        0.0      0.0    5814.0   \n",
            "betas[8]  2.068  0.006   2.058    2.078        0.0      0.0   10586.0   \n",
            "sigma     0.720  0.004   0.712    0.727        0.0      0.0    9287.0   \n",
            "\n",
            "          ess_tail  r_hat  \n",
            "betas[0]    5555.0    1.0  \n",
            "betas[1]    6287.0    1.0  \n",
            "betas[2]    5165.0    1.0  \n",
            "betas[3]    5115.0    1.0  \n",
            "betas[4]    5937.0    1.0  \n",
            "betas[5]    5824.0    1.0  \n",
            "betas[6]    5253.0    1.0  \n",
            "betas[7]    4922.0    1.0  \n",
            "betas[8]    5920.0    1.0  \n",
            "sigma       5881.0    1.0  \n",
            "\n",
            "Generating trace plots...\n",
            "Generating posterior distribution plots...\n",
            "\n",
            "Rhat statistics for convergence assessment:\n",
            "Max Rhat: 1.0008336071311184\n",
            "Min Rhat: 1.0008336071311184\n",
            "\n",
            "Effective Sample Size (ESS):\n",
            "Minimum ESS: 9286.98212196795\n",
            "\n",
            "Making predictions with the model...\n",
            "Generating predictions on test set...\n",
            "Test set RMSE: 0.7456\n",
            "\n",
            "Feature importance analysis:\n",
            "      Feature  Coefficient\n",
            "8   intercept     2.067814\n",
            "6    Latitude    -0.896080\n",
            "7   Longitude    -0.868330\n",
            "0      MedInc     0.852487\n",
            "3   AveBedrms     0.371168\n",
            "2    AveRooms    -0.305263\n",
            "1    HouseAge     0.122462\n",
            "5    AveOccup    -0.036706\n",
            "4  Population    -0.002213\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea3f6e27fe484af69bce33ff19433121"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Performing posterior predictive checks...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Could not generate PPC plot - version incompatibility: module 'arviz' has no attribute 'from_pymc'\n",
            "\n",
            "Credible intervals for coefficients:\n",
            "MedInc: 0.8525 [0.8349, 0.8702]\n",
            "HouseAge: 0.1225 [0.1106, 0.1347]\n",
            "AveRooms: -0.3053 [-0.3380, -0.2730]\n",
            "AveBedrms: 0.3712 [0.3403, 0.4019]\n",
            "Population: -0.0022 [-0.0138, 0.0093]\n",
            "AveOccup: -0.0367 [-0.0466, -0.0270]\n",
            "Latitude: -0.8961 [-0.9295, -0.8630]\n",
            "Longitude: -0.8683 [-0.9015, -0.8363]\n",
            "intercept: 2.0678 [2.0570, 2.0786]\n",
            "\n",
            "Bayesian Linear Regression analysis complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3: perform robust Bayesian Linear Regression\n",
        "\n",
        "Let $p(\\tau_i)$ be $\\require{cancel} \\textrm{gamma}\\big(\\tau_i | \\alpha = \\frac{\\nu}{2}, \\overset{\\textrm{rate}\\xcancel{\\textrm{scale}}}{\\beta = \\frac{\\nu}{2}}\\big)$ and let $p(y_i|\\boldsymbol \\beta, \\tau,\\tau_i)$ be $\\mathcal{N}(y_i | \\textbf{X} \\boldsymbol \\beta, \\overset{\\textrm{precision}}{\\tau \\times \\tau_i})$.\n",
        "\n",
        "- Return to your dataset and maniputate it to have some various outliers or find another dataset with some outlier data\n",
        "- Use an appropriate prior for inference on $v$ if you have enough data to do so\n",
        "- Use the posterior distributions of the $\\tau_i$'s to identify data point \"outliers\"\n",
        "- Use the posterior distributions of the $\\sigma_i^{-2} = \\tau \\times \\tau_i$ to create posterior distribuitions of the **influence** (the diagonals of the $H$ \"hat\" matrix $X^\\top (X^\\top D X)^{-1} X$ where $D_{ij}=0$ and $D_{ii} = \\sigma^2_i$) and compare and contras some example \"outlier\" versus \"non outlier\" data points\n",
        "\n",
        "- Provide inference with Bayesian posterior analysis and report MCMC diagnostics\n",
        "\n",
        "#### [Optional] Q3 Extra: measurement error models?\n",
        "\n",
        "What if $\\textbf{x}_i = \\textbf{x}_i^{true} + \\eta_i, \\eta_i \\sim \\mathcal{MVN}(\\textbf{0}, \\Sigma)$ for some kind of measurement error covariance structure $\\Sigma$ and $\\mathcal N (y_i| \\textbf{X}^{true}\\boldsymbol \\beta, \\sigma)$?"
      ],
      "metadata": {
        "id": "0lfRBBZyBRQG"
      },
      "id": "0lfRBBZyBRQG"
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3 Answer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pymc as pm\n",
        "import arviz as az\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "import warnings\n",
        "\n",
        "# Filter some warnings for cleaner output\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Load and prepare the California Housing dataset\n",
        "print(\"Loading California Housing dataset...\")\n",
        "housing = fetch_california_housing()\n",
        "X_raw = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
        "y_raw = housing.target\n",
        "\n",
        "# We'll focus on a subset of features to simplify the model\n",
        "selected_features = ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population']\n",
        "X = X_raw[selected_features].copy()\n",
        "\n",
        "# Create a version of the dataset with artificial outliers\n",
        "print(\"Creating dataset with artificial outliers...\")\n",
        "y = y_raw.copy()\n",
        "\n",
        "# Introduce some outliers by multiplying some values by a factor\n",
        "n_outliers = 50  # Number of outliers to introduce\n",
        "outlier_indices = np.random.choice(len(y), n_outliers, replace=False)\n",
        "outlier_factor = 3.0  # How much to scale outlier values\n",
        "\n",
        "# Store the original values for comparison\n",
        "y_original = y.copy()\n",
        "\n",
        "# Create outliers\n",
        "y[outlier_indices] *= outlier_factor\n",
        "print(f\"Introduced {n_outliers} outliers with scaling factor {outlier_factor}\")\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
        "\n",
        "# Add constant term for intercept\n",
        "X_scaled['intercept'] = 1.0\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42, shuffle=True\n",
        ")\n",
        "\n",
        "# Keep track of which indices are outliers in the training set\n",
        "train_indices = np.arange(len(y))[:-len(y_test)]\n",
        "outlier_mask_train = np.isin(train_indices, outlier_indices)\n",
        "\n",
        "print(f\"\\nTraining set size: {X_train.shape}\")\n",
        "print(f\"Number of outliers in training set: {np.sum(outlier_mask_train)}\")\n",
        "\n",
        "# Prepare data for PyMC\n",
        "p = X_train.shape[1]  # Number of predictors including intercept\n",
        "n = X_train.shape[0]  # Number of observations\n",
        "\n",
        "# Implement Robust Bayesian Linear Regression\n",
        "print(\"\\nBuilding Robust Bayesian Linear Regression model...\")\n",
        "with pm.Model() as robust_model:\n",
        "    # Hyperprior for degrees of freedom (v)\n",
        "    # This controls the heaviness of the tails in the t-distribution\n",
        "    v = pm.Gamma('v', alpha=3, beta=0.1)\n",
        "\n",
        "    # Global precision parameter\n",
        "    tau = pm.Gamma('tau', alpha=1, beta=1)\n",
        "\n",
        "    # Individual precision scaling factors for each observation\n",
        "    tau_i = pm.Gamma('tau_i', alpha=v/2, beta=v/2, shape=n)\n",
        "\n",
        "    # Prior for regression coefficients\n",
        "    betas = pm.Normal('betas', mu=0, sigma=1, shape=p)\n",
        "\n",
        "    # Expected value of outcome\n",
        "    mu = pm.math.dot(X_train, betas)\n",
        "\n",
        "    # Likelihood with varying precisions for each observation\n",
        "    y_obs = pm.Normal('y', mu=mu, tau=tau * tau_i, observed=y_train)\n",
        "\n",
        "    # Sample from the posterior\n",
        "    print(\"Sampling from posterior distribution...\")\n",
        "    trace = pm.sample(\n",
        "        draws=1000,\n",
        "        tune=500,\n",
        "        chains=4,\n",
        "        cores=2,\n",
        "        target_accept=0.9,\n",
        "        return_inferencedata=True\n",
        "    )\n",
        "\n",
        "# Examine MCMC diagnostics\n",
        "print(\"\\nMCMC Diagnostics:\")\n",
        "summary = az.summary(trace, var_names=['betas', 'tau', 'v'])\n",
        "print(summary)\n",
        "\n",
        "# Plot trace for main parameters\n",
        "print(\"\\nGenerating trace plots...\")\n",
        "az.plot_trace(trace, var_names=['betas', 'tau', 'v'])\n",
        "plt.tight_layout()\n",
        "plt.savefig('robust_trace_plot.png')\n",
        "plt.close()\n",
        "\n",
        "# Plot posterior distributions\n",
        "print(\"Generating posterior distribution plots...\")\n",
        "az.plot_posterior(trace, var_names=['betas', 'tau', 'v'])\n",
        "plt.tight_layout()\n",
        "plt.savefig('robust_posterior_plot.png')\n",
        "plt.close()\n",
        "\n",
        "# Extract posterior samples\n",
        "beta_samples = trace.posterior['betas'].values  # Shape: (chains, draws, p)\n",
        "tau_samples = trace.posterior['tau'].values     # Shape: (chains, draws)\n",
        "tau_i_samples = trace.posterior['tau_i'].values # Shape: (chains, draws, n)\n",
        "v_samples = trace.posterior['v'].values         # Shape: (chains, draws)\n",
        "\n",
        "# Calculate posterior means\n",
        "beta_means = beta_samples.mean(axis=(0, 1))\n",
        "tau_mean = tau_samples.mean()\n",
        "tau_i_means = tau_i_samples.mean(axis=(0, 1))\n",
        "v_mean = v_samples.mean()\n",
        "\n",
        "print(f\"\\nPosterior mean of degrees of freedom (v): {v_mean:.2f}\")\n",
        "print(f\"Posterior mean of global precision (tau): {tau_mean:.2f}\")\n",
        "\n",
        "# Analyze tau_i values to identify outliers\n",
        "print(\"\\nAnalyzing individual precision factors (tau_i) to identify outliers...\")\n",
        "\n",
        "# Compute scaled precision for each observation (sigma_i^-2 = tau × tau_i)\n",
        "sigma_inv_sq = np.outer(tau_samples.flatten(), tau_i_means)\n",
        "sigma_inv_sq_mean = sigma_inv_sq.mean(axis=0)\n",
        "\n",
        "# Sort observations by their tau_i values\n",
        "tau_i_df = pd.DataFrame({\n",
        "    'index': np.arange(n),\n",
        "    'tau_i': tau_i_means,\n",
        "    'is_outlier': outlier_mask_train,\n",
        "    'y_value': y_train,\n",
        "    'sigma_inv_sq': sigma_inv_sq_mean\n",
        "})\n",
        "\n",
        "# Sort by tau_i (lower values suggest outliers)\n",
        "tau_i_df_sorted = tau_i_df.sort_values('tau_i')\n",
        "\n",
        "# Plot tau_i distribution\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(tau_i_df['tau_i'], bins=30, kde=True)\n",
        "plt.axvline(tau_i_df[tau_i_df['is_outlier']]['tau_i'].median(), color='red', linestyle='--',\n",
        "            label='Median τᵢ for true outliers')\n",
        "plt.axvline(tau_i_df[~tau_i_df['is_outlier']]['tau_i'].median(), color='green', linestyle='--',\n",
        "            label='Median τᵢ for normal points')\n",
        "plt.title('Distribution of Precision Scaling Factors (τᵢ)')\n",
        "plt.xlabel('τᵢ Value')\n",
        "plt.legend()\n",
        "plt.savefig('tau_i_distribution.png')\n",
        "plt.close()\n",
        "\n",
        "# Print top potential outliers\n",
        "print(\"\\nTop 10 potential outliers based on lowest τᵢ values:\")\n",
        "print(tau_i_df_sorted.head(10)[['index', 'tau_i', 'is_outlier', 'y_value']])\n",
        "\n",
        "# Print representative non-outliers\n",
        "print(\"\\nRepresentative non-outliers based on highest τᵢ values:\")\n",
        "print(tau_i_df_sorted.tail(10)[['index', 'tau_i', 'is_outlier', 'y_value']])\n",
        "\n",
        "# Calculate the ROC curve for outlier detection using tau_i\n",
        "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
        "\n",
        "# Use -tau_i as the score (lower tau_i = more likely to be outlier)\n",
        "fpr, tpr, _ = roc_curve(outlier_mask_train, -tau_i_means)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve for Outlier Detection Using τᵢ')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.savefig('outlier_detection_roc.png')\n",
        "plt.close()\n",
        "\n",
        "# Calculate precision-recall curve\n",
        "precision, recall, _ = precision_recall_curve(outlier_mask_train, -tau_i_means)\n",
        "pr_auc = auc(recall, precision)\n",
        "\n",
        "# Plot precision-recall curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, color='darkorange', lw=2, label=f'PR curve (area = {pr_auc:.2f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve for Outlier Detection')\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.savefig('outlier_detection_pr.png')\n",
        "plt.close()\n",
        "\n",
        "# Compute influence metrics\n",
        "print(\"\\nCalculating influence metrics...\")\n",
        "\n",
        "# Compute diagonal elements of the hat matrix using posterior samples\n",
        "def compute_influence(X, sigma_inv_sq):\n",
        "    \"\"\"\n",
        "    Compute the diagonal elements of the hat matrix (influence)\n",
        "    H = X(X'DX)^-1X' where D is diagonal with D_ii = sigma_i^2\n",
        "    Here we use sigma_inv_sq = 1/sigma_i^2 = tau * tau_i\n",
        "    \"\"\"\n",
        "    D = np.diag(1.0 / sigma_inv_sq)  # Convert precision to variance\n",
        "    XtDX_inv = np.linalg.inv(X.T @ D @ X)\n",
        "    H_diag = np.zeros(X.shape[0])\n",
        "\n",
        "    for i in range(X.shape[0]):\n",
        "        x_i = X[i:i+1]  # 1 x p\n",
        "        H_diag[i] = x_i @ XtDX_inv @ x_i.T\n",
        "\n",
        "    return H_diag\n",
        "\n",
        "# Compute influence for a subset of posterior samples\n",
        "n_samples = 100\n",
        "influence_samples = np.zeros((n_samples, n))\n",
        "\n",
        "# For each sample, compute influence\n",
        "for i in range(n_samples):\n",
        "    # Sample index from flattened posterior\n",
        "    idx = np.random.randint(0, tau_samples.size)\n",
        "\n",
        "    # Get tau value for this sample\n",
        "    tau_sample = tau_samples.flatten()[idx]\n",
        "\n",
        "    # Get tau_i values for this sample (using the mean across samples for simplicity)\n",
        "    # In a full analysis, we'd use corresponding tau_i samples\n",
        "    sigma_inv_sq_sample = tau_sample * tau_i_means\n",
        "\n",
        "    # Compute influence\n",
        "    influence_samples[i] = compute_influence(X_train.values, sigma_inv_sq_sample)\n",
        "\n",
        "# Average influence across samples\n",
        "influence_mean = influence_samples.mean(axis=0)\n",
        "\n",
        "# Add influence to dataframe\n",
        "tau_i_df['influence'] = influence_mean\n",
        "\n",
        "# Plot influence vs tau_i\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(tau_i_df['tau_i'], tau_i_df['influence'], c=tau_i_df['is_outlier'],\n",
        "            cmap='coolwarm', alpha=0.7)\n",
        "plt.xlabel('Precision Scaling Factor (τᵢ)')\n",
        "plt.ylabel('Influence (diagonal of hat matrix)')\n",
        "plt.title('Relationship Between Precision Scaling and Influence')\n",
        "plt.colorbar(label='Is True Outlier')\n",
        "plt.savefig('influence_vs_tau_i.png')\n",
        "plt.close()\n",
        "\n",
        "# Compare outliers vs non-outliers\n",
        "print(\"\\nComparing outliers vs non-outliers:\")\n",
        "print(f\"Average τᵢ for outliers: {tau_i_df[tau_i_df['is_outlier']]['tau_i'].mean():.4f}\")\n",
        "print(f\"Average τᵢ for non-outliers: {tau_i_df[~tau_i_df['is_outlier']]['tau_i'].mean():.4f}\")\n",
        "print(f\"Average influence for outliers: {tau_i_df[tau_i_df['is_outlier']]['influence'].mean():.4f}\")\n",
        "print(f\"Average influence for non-outliers: {tau_i_df[~tau_i_df['is_outlier']]['influence'].mean():.4f}\")\n",
        "\n",
        "# Select a few example outliers and non-outliers for detailed analysis\n",
        "top_outliers = tau_i_df_sorted.head(5)['index'].values\n",
        "top_non_outliers = tau_i_df_sorted.tail(5)['index'].values\n",
        "\n",
        "# Plot posterior distributions of tau_i for these examples\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# For outliers\n",
        "for idx, obs_idx in enumerate(top_outliers):\n",
        "    # Extract posterior samples for this observation\n",
        "    tau_i_posterior = tau_i_samples[:, :, obs_idx].flatten()\n",
        "    sns.kdeplot(tau_i_posterior, label=f'Outlier {obs_idx}', linestyle='--')\n",
        "\n",
        "# For non-outliers\n",
        "for idx, obs_idx in enumerate(top_non_outliers):\n",
        "    # Extract posterior samples for this observation\n",
        "    tau_i_posterior = tau_i_samples[:, :, obs_idx].flatten()\n",
        "    sns.kdeplot(tau_i_posterior, label=f'Non-outlier {obs_idx}')\n",
        "\n",
        "plt.title('Posterior Distributions of τᵢ for Selected Observations')\n",
        "plt.xlabel('τᵢ Value')\n",
        "plt.legend()\n",
        "plt.savefig('tau_i_posterior_examples.png')\n",
        "plt.close()\n",
        "\n",
        "# Plot posterior distributions of influence for these examples\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# For each observation, compute influence across samples\n",
        "for idx, obs_idx in enumerate(np.concatenate([top_outliers, top_non_outliers])):\n",
        "    is_outlier = obs_idx in top_outliers\n",
        "    label = f'{\"Outlier\" if is_outlier else \"Non-outlier\"} {obs_idx}'\n",
        "    style = '--' if is_outlier else '-'\n",
        "\n",
        "    # Extract influence values for this observation\n",
        "    influence_posterior = influence_samples[:, obs_idx]\n",
        "    sns.kdeplot(influence_posterior, label=label, linestyle=style)\n",
        "\n",
        "plt.title('Posterior Distributions of Influence for Selected Observations')\n",
        "plt.xlabel('Influence Value')\n",
        "plt.legend()\n",
        "plt.savefig('influence_posterior_examples.png')\n",
        "plt.close()\n",
        "\n",
        "# Generate predictions\n",
        "print(\"\\nGenerating predictions and assessing model performance...\")\n",
        "\n",
        "# Function to make predictions with beta values\n",
        "def predict(X, betas):\n",
        "    return X @ betas\n",
        "\n",
        "# Make predictions on test set\n",
        "y_pred_samples = np.zeros((beta_samples.shape[0], beta_samples.shape[1], len(X_test)))\n",
        "for i in range(beta_samples.shape[0]):  # Chains\n",
        "    for j in range(beta_samples.shape[1]):  # Draws\n",
        "        y_pred_samples[i, j] = predict(X_test.values, beta_samples[i, j])\n",
        "\n",
        "# Reshape y_pred_samples for az.hdi\n",
        "y_pred_samples_reshaped = y_pred_samples.reshape((-1, len(y_test))).T\n",
        "\n",
        "# Calculate mean and credible intervals\n",
        "y_pred_mean = np.mean(y_pred_samples_reshaped, axis=1)\n",
        "y_pred_hdi = az.hdi(y_pred_samples_reshaped, hdi_prob=0.95)\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(np.mean((y_pred_mean - y_test) ** 2))\n",
        "print(f\"Test set RMSE: {rmse:.4f}\")\n",
        "\n",
        "# Plot actual vs predicted values\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_test, y_pred_mean, alpha=0.7)\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title('Actual vs. Predicted Values (Test Set)')\n",
        "plt.savefig('actual_vs_predicted_test.png')\n",
        "plt.close()\n",
        "\n",
        "# Plot credible intervals\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_test, y_pred_mean, alpha=0.7, label='Predicted Mean')\n",
        "plt.plot(y_test, label='Actual Values', linestyle='--')\n",
        "plt.xlabel('Test Data Points')\n",
        "plt.ylabel('Values')\n",
        "plt.title('Predicted vs. Actual Values with Credible Intervals (Test Set)')\n",
        "plt.legend()\n",
        "plt.savefig('predicted_vs_actual_hdi.png')\n",
        "plt.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c4930652c5294fc39304a69e1667671c",
            "2f27d42f4e664eb691c880b6a8f51d0b"
          ]
        },
        "id": "p4nSuz2wINvT",
        "outputId": "73f9a884-84ec-41a6-a87d-a3c88468d059"
      },
      "id": "p4nSuz2wINvT",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading California Housing dataset...\n",
            "Creating dataset with artificial outliers...\n",
            "Introduced 50 outliers with scaling factor 3.0\n",
            "\n",
            "Training set size: (16512, 6)\n",
            "Number of outliers in training set: 36\n",
            "\n",
            "Building Robust Bayesian Linear Regression model...\n",
            "Sampling from posterior distribution...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c4930652c5294fc39304a69e1667671c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:pymc.stats.convergence:The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MCMC Diagnostics:\n",
            "           mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  \\\n",
            "betas[0]  1.160  0.009   1.143    1.177      0.000    0.000    1357.0   \n",
            "betas[1]  0.175  0.006   0.164    0.185      0.000    0.000    2718.0   \n",
            "betas[2] -0.714  0.017  -0.746   -0.683      0.000    0.000    1316.0   \n",
            "betas[3]  0.655  0.018   0.621    0.689      0.000    0.000    1364.0   \n",
            "betas[4]  0.021  0.005   0.012    0.030      0.000    0.000    3555.0   \n",
            "betas[5]  1.961  0.005   1.951    1.971      0.000    0.000    1404.0   \n",
            "tau       3.706  0.076   3.562    3.850      0.005    0.004     225.0   \n",
            "v         3.198  0.091   3.019    3.358      0.008    0.005     139.0   \n",
            "\n",
            "          ess_tail  r_hat  \n",
            "betas[0]    2559.0   1.00  \n",
            "betas[1]    2925.0   1.00  \n",
            "betas[2]    2049.0   1.00  \n",
            "betas[3]    2197.0   1.00  \n",
            "betas[4]    2535.0   1.00  \n",
            "betas[5]    2292.0   1.00  \n",
            "tau          931.0   1.02  \n",
            "v            560.0   1.03  \n",
            "\n",
            "Generating trace plots...\n",
            "Generating posterior distribution plots...\n",
            "\n",
            "Posterior mean of degrees of freedom (v): 3.20\n",
            "Posterior mean of global precision (tau): 3.71\n",
            "\n",
            "Analyzing individual precision factors (tau_i) to identify outliers...\n",
            "\n",
            "Top 10 potential outliers based on lowest τᵢ values:\n",
            "       index     tau_i  is_outlier  y_value\n",
            "13159  13159  0.011894       False  5.00001\n",
            "12566  12566  0.022262       False  1.31300\n",
            "2292    2292  0.032365       False  1.12500\n",
            "9079    9079  0.042113       False  1.37500\n",
            "13258  13258  0.043943       False  4.37500\n",
            "3992    3992  0.045951       False  4.20000\n",
            "13836  13836  0.053114       False  1.12500\n",
            "13385  13385  0.054249       False  0.67500\n",
            "7604    7604  0.055515       False  5.00001\n",
            "14133  14133  0.056146       False  4.75000\n",
            "\n",
            "Representative non-outliers based on highest τᵢ values:\n",
            "       index     tau_i  is_outlier  y_value\n",
            "12166  12166  1.341364       False    1.516\n",
            "13002  13002  1.342642       False    2.608\n",
            "4443    4443  1.343223       False    0.942\n",
            "4605    4605  1.343754       False    1.791\n",
            "9873    9873  1.343768       False    1.405\n",
            "15642  15642  1.344752       False    1.755\n",
            "12670  12670  1.345284       False    2.556\n",
            "14577  14577  1.346106       False    1.431\n",
            "14832  14832  1.348627       False    2.171\n",
            "899      899  1.348651       False    1.784\n",
            "\n",
            "Calculating influence metrics...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-422ae01d61fa>:226: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  H_diag[i] = x_i @ XtDX_inv @ x_i.T\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Comparing outliers vs non-outliers:\n",
            "Average τᵢ for outliers: 1.0914\n",
            "Average τᵢ for non-outliers: 0.9999\n",
            "Average influence for outliers: 0.0006\n",
            "Average influence for non-outliers: 0.0007\n",
            "\n",
            "Generating predictions and assessing model performance...\n",
            "Test set RMSE: 1.0044\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a7bccd654d17491f8cf1671e72c5692d": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_f7b1016132434694a90270020b81a5b8",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "                                                                                                                   \n \u001b[1m \u001b[0m\u001b[1mProgress                 \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mDraws\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mDivergences\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mStep size\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mGrad evals\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mSampling Speed\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mElapsed\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mRemaining\u001b[0m\u001b[1m \u001b[0m \n ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n  \u001b[38;2;31;119;180m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   3000    0             0.27        15           134.20 draws/s   0:00:22   0:00:00    \n  \u001b[38;2;31;119;180m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   3000    0             0.27        15           130.69 draws/s   0:00:22   0:00:00    \n  \u001b[38;2;31;119;180m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   3000    0             0.33        15           69.43 draws/s    0:00:43   0:00:00    \n  \u001b[38;2;31;119;180m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   3000    0             0.30        15           68.97 draws/s    0:00:43   0:00:00    \n                                                                                                                   \n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                                                                                                   \n <span style=\"font-weight: bold\"> Progress                  </span> <span style=\"font-weight: bold\"> Draws </span> <span style=\"font-weight: bold\"> Divergences </span> <span style=\"font-weight: bold\"> Step size </span> <span style=\"font-weight: bold\"> Grad evals </span> <span style=\"font-weight: bold\"> Sampling Speed </span> <span style=\"font-weight: bold\"> Elapsed </span> <span style=\"font-weight: bold\"> Remaining </span> \n ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n  <span style=\"color: #1f77b4; text-decoration-color: #1f77b4\">━━━━━━━━━━━━━━━━━━━━━━━━━</span>   3000    0             0.27        15           134.20 draws/s   0:00:22   0:00:00    \n  <span style=\"color: #1f77b4; text-decoration-color: #1f77b4\">━━━━━━━━━━━━━━━━━━━━━━━━━</span>   3000    0             0.27        15           130.69 draws/s   0:00:22   0:00:00    \n  <span style=\"color: #1f77b4; text-decoration-color: #1f77b4\">━━━━━━━━━━━━━━━━━━━━━━━━━</span>   3000    0             0.33        15           69.43 draws/s    0:00:43   0:00:00    \n  <span style=\"color: #1f77b4; text-decoration-color: #1f77b4\">━━━━━━━━━━━━━━━━━━━━━━━━━</span>   3000    0             0.30        15           68.97 draws/s    0:00:43   0:00:00    \n                                                                                                                   \n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "f7b1016132434694a90270020b81a5b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea3f6e27fe484af69bce33ff19433121": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_75cd4bce1f7a4c3a9fe719ea5e7f768b",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "Sampling ... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m 0:00:00 / 0:00:03\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Sampling ... <span style=\"color: #008000; text-decoration-color: #008000\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> 0:00:00 / 0:00:03\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "75cd4bce1f7a4c3a9fe719ea5e7f768b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4930652c5294fc39304a69e1667671c": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_2f27d42f4e664eb691c880b6a8f51d0b",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "                                                                                                                   \n \u001b[1m \u001b[0m\u001b[1mProgress                 \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mDraws\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mDivergences\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mStep size\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mGrad evals\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mSampling Speed\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mElapsed\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mRemaining\u001b[0m\u001b[1m \u001b[0m \n ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n  \u001b[38;2;31;119;180m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   1500    0             0.13        31           8.51 draws/s     0:02:56   0:00:00    \n  \u001b[38;2;31;119;180m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   1500    0             0.12        31           8.65 draws/s     0:02:53   0:00:00    \n  \u001b[38;2;31;119;180m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   1500    0             0.12        31           4.34 draws/s     0:05:45   0:00:00    \n  \u001b[38;2;31;119;180m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   1500    0             0.11        31           4.29 draws/s     0:05:49   0:00:00    \n                                                                                                                   \n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                                                                                                   \n <span style=\"font-weight: bold\"> Progress                  </span> <span style=\"font-weight: bold\"> Draws </span> <span style=\"font-weight: bold\"> Divergences </span> <span style=\"font-weight: bold\"> Step size </span> <span style=\"font-weight: bold\"> Grad evals </span> <span style=\"font-weight: bold\"> Sampling Speed </span> <span style=\"font-weight: bold\"> Elapsed </span> <span style=\"font-weight: bold\"> Remaining </span> \n ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n  <span style=\"color: #1f77b4; text-decoration-color: #1f77b4\">━━━━━━━━━━━━━━━━━━━━━━━━━</span>   1500    0             0.13        31           8.51 draws/s     0:02:56   0:00:00    \n  <span style=\"color: #1f77b4; text-decoration-color: #1f77b4\">━━━━━━━━━━━━━━━━━━━━━━━━━</span>   1500    0             0.12        31           8.65 draws/s     0:02:53   0:00:00    \n  <span style=\"color: #1f77b4; text-decoration-color: #1f77b4\">━━━━━━━━━━━━━━━━━━━━━━━━━</span>   1500    0             0.12        31           4.34 draws/s     0:05:45   0:00:00    \n  <span style=\"color: #1f77b4; text-decoration-color: #1f77b4\">━━━━━━━━━━━━━━━━━━━━━━━━━</span>   1500    0             0.11        31           4.29 draws/s     0:05:49   0:00:00    \n                                                                                                                   \n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "2f27d42f4e664eb691c880b6a8f51d0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}